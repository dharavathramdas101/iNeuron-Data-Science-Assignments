{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182684df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1. After each stride-2 conv, why do we double the number of filters?**\n",
    "\n",
    "After each stride-2 convolution, we double the number of filters because we want to learn more features. Stride-2 convolutions reduce the size of the output activation, so we need to increase the number of filters to compensate. This is because the number of features that can be learned is proportional to the size of the activation.\n",
    "\n",
    "For example, if we have an input image with a size of 28x28 and we use a stride-2 convolution with a kernel size of 3x3, the output activation will have a size of 14x14. If we keep the number of filters the same, we will only be able to learn 14x14 = 196 features. However, if we double the number of filters, we will be able to learn 2*196 = 392 features.\n",
    "\n",
    "**2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?**\n",
    "\n",
    "We use a larger kernel with MNIST (with simple CNN) in the first conv because we want to learn more general features. The first convolution layer is responsible for learning the most basic features in the image, such as edges and corners. By using a larger kernel, we can learn more general features that can be applied to a wider variety of images.\n",
    "\n",
    "For example, if we use a kernel size of 3x3 in the first convolution layer, we can only learn features that are 3 pixels apart. However, if we use a kernel size of 5x5, we can learn features that are 5 pixels apart. This means that the first convolution layer will be able to learn more general features that can be applied to a wider variety of images.\n",
    "\n",
    "**3. What data is saved by ActivationStats for each layer?**\n",
    "\n",
    "ActivationStats saves the following data for each layer:\n",
    "\n",
    "* The mean activation of the layer\n",
    "* The standard deviation of the activation of the layer\n",
    "* The minimum activation of the layer\n",
    "* The maximum activation of the layer\n",
    "* The number of activations that are above a certain threshold\n",
    "\n",
    "This data can be used to track the progress of training and to identify potential problems with the model. For example, if the mean activation of a layer is very low, it may indicate that the layer is not learning anything. Similarly, if the standard deviation of a layer is very low, it may indicate that the layer is not learning any new information.\n",
    "\n",
    "**4. How do we get a learner&#39;s callback after they&#39;ve completed training?**\n",
    "\n",
    "To get a learner's callback after they have completed training, you can use the `learner.on_completed()` method. This method takes a callback function as an argument. The callback function will be called after the learner has completed training.\n",
    "\n",
    "For example, the following code shows how to get a learner's callback after they have completed training:\n",
    "\n",
    "```\n",
    "import fastai\n",
    "\n",
    "def my_callback(learner):\n",
    "  print(\"Training completed!\")\n",
    "\n",
    "learner = fastai.Learner()\n",
    "learner.on_completed(my_callback)\n",
    "learner.fit()\n",
    "```\n",
    "\n",
    "**5. What are the drawbacks of activations above zero?**\n",
    "\n",
    "Activations above zero can cause the model to become unstable. This is because the model may start to learn features that are not relevant to the task at hand. For example, if the model is trying to classify images of cats and dogs, it may start to learn features that are specific to one particular cat or dog. This can make it difficult for the model to generalize to new images of cats and dogs.\n",
    "\n",
    "**6. Draw up the benefits and drawbacks of practicing in larger batches?**\n",
    "\n",
    "Training in larger batches can have both benefits and drawbacks.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Training in larger batches can be more efficient. This is because the model can process more data in each iteration.\n",
    "* Training in larger batches can help to reduce overfitting. This is because the model is less likely to learn features that are specific to a particular batch of data.\n",
    "\n",
    "**Drawbacks:**\n",
    "\n",
    "* Training in larger batches can be more difficult to debug. This is because it can be more difficult to identify the source of a problem when the model is trained on a large amount of data.\n",
    "* Training in larger batches can require more memory. This is because the model needs to store the entire batch of data in memory before it can start training.\n",
    "\n",
    "\n",
    "\n",
    "**7. Why should we avoid starting training with a high learning rate?**\n",
    "\n",
    "We should avoid starting training with a high learning rate because it can cause the model to diverge. This is because the model may make large changes to its weights in each iteration, which can lead to the model becoming unstable.\n",
    "\n",
    "A high learning rate can also cause the model to overfit the training data. This is because the model may learn the noise in the training data, which can make it difficult for the model to generalize to new data.\n",
    "\n",
    "**8. What are the pros of studying with a high rate of learning?**\n",
    "\n",
    "The pros of studying with a high rate of learning include:\n",
    "\n",
    "* The model can learn faster.\n",
    "* The model can find the optimal solution more quickly.\n",
    "* The model can be more robust to noise in the training data.\n",
    "\n",
    "**9. Why do we want to end the training with a low learning rate?**\n",
    "\n",
    "We want to end the training with a low learning rate because it helps the model to converge. This is because the model will make smaller changes to its weights in each iteration, which will help the model to find a stable solution.\n",
    "\n",
    "A low learning rate can also help the model to generalize to new data. This is because the model will not be as sensitive to noise in the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40eacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
