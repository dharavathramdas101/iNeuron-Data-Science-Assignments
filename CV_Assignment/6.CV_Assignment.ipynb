{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0fefc3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?**\n",
    "\n",
    "Trainable parameters are the weights and biases of the model that are updated during training. These parameters are responsible for learning the relationships between the input and output data. Non-trainable parameters are the parameters of the model that are not updated during training. These parameters are typically used for normalization or regularization purposes.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "* In a convolutional neural network (CNN), the weights and biases of the convolutional layers are trainable parameters. The mean and variance of the activations in the convolutional layers are non-trainable parameters.\n",
    "* In a recurrent neural network (RNN), the weights and biases of the RNN cells are trainable parameters. The initial state of the RNN is a non-trainable parameter.\n",
    "\n",
    "**2. In the CNN architecture, where does the DROPOUT LAYER go?**\n",
    "\n",
    "The dropout layer typically goes between the hidden layers of a CNN. It is a regularization technique that helps to prevent overfitting by randomly dropping out (setting to zero) a certain percentage of the neurons in the network during each iteration.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "* If you have a CNN with two hidden layers, you could put the dropout layer between the two hidden layers. This would mean that a certain percentage of the neurons in the first hidden layer would be randomly dropped out, and then a certain percentage of the neurons in the second hidden layer would be randomly dropped out.\n",
    "\n",
    "**3. What is the optimal number of hidden layers to stack?**\n",
    "\n",
    "The optimal number of hidden layers to stack depends on the specific task that you are trying to solve. Generally, a deeper network (with more hidden layers) will be able to learn more complex relationships, but it will also be more prone to overfitting. A shallower network (with fewer hidden layers) will be less prone to overfitting, but it may not be able to learn as complex relationships.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "* If you are trying to classify images, you may need a deeper network with more hidden layers. However, if you are trying to predict the next word in a sentence, you may be able to get away with a shallower network with fewer hidden layers.\n",
    "\n",
    "**4. In each layer, how many secret units or filters should there be?**\n",
    "\n",
    "The number of secret units or filters in each layer depends on the specific task that you are trying to solve and the size of your dataset. Generally, a larger number of secret units or filters will allow the network to learn more complex relationships, but it will also require more training data.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "* If you are trying to classify images with a small dataset, you may want to use a smaller number of secret units or filters. However, if you are trying to classify images with a large dataset, you may be able to get away with using a larger number of secret units or filters.\n",
    "\n",
    "**5. What should your initial learning rate be?**\n",
    "\n",
    "The initial learning rate is the rate at which the weights of the model are updated during training. A high learning rate can help the model to learn quickly, but it can also lead to overfitting. A low learning rate can help the model to avoid overfitting, but it can also make the training process slower.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "* A good starting point for the initial learning rate is 0.01. You can then experiment with different learning rates to find the optimal value for your specific task.\n",
    "\n",
    "\n",
    "**6. What do you do with the activation function?**\n",
    "\n",
    "An activation function is a function that is applied to the output of a neuron in a neural network. It transforms the output of the neuron into a new value that can be used by other neurons in the network.\n",
    "\n",
    "There are many different activation functions available, each with its own properties. Some common activation functions include:\n",
    "\n",
    "* **ReLU:** The rectified linear unit (ReLU) activation function is one of the most common activation functions used in neural networks. It is a non-linear function that outputs the input value if it is positive, and outputs 0 if it is negative.\n",
    "[Image of ReLU activation function]\n",
    "* **Sigmoid:** The sigmoid activation function is a non-linear function that outputs a value between 0 and 1. It is often used in classification tasks, where the output of the network is a probability.\n",
    "[Image of Sigmoid activation function]\n",
    "* **Tanh:** The hyperbolic tangent (tanh) activation function is similar to the sigmoid activation function, but it outputs a value between -1 and 1. It is often used in regression tasks, where the output of the network is a continuous value.\n",
    "[Image of Tanh activation function]\n",
    "\n",
    "The activation function is important because it determines how the output of the neurons is interpreted. For example, the ReLU activation function allows the output of the neurons to be positive or zero, while the sigmoid activation function allows the output of the neurons to be between 0 and 1.\n",
    "\n",
    "**7. What is NORMALIZATION OF DATA?**\n",
    "\n",
    "Normalization of data is a technique that is used to standardize the data before it is used to train a model. This can help to improve the performance of the model by making the data more consistent.\n",
    "\n",
    "There are many different ways to normalize data. A common approach is to subtract the mean and divide by the standard deviation of each feature. This can help to center the data around zero and to scale the data so that each feature has a similar range of values.\n",
    "\n",
    "**8. What is IMAGE AUGMENTATION and how does it work?**\n",
    "\n",
    "Image augmentation is a technique that is used to artificially increase the size of the training dataset. This can help to prevent overfitting by exposing the model to more data.\n",
    "\n",
    "Image augmentation can be done by applying a variety of transformations to the training data, such as flipping, rotating, and cropping images. This can help to make the model more robust to variations in the input data.\n",
    "\n",
    "For example, if you are training a model to classify images of cats, you could augment the dataset by flipping the images horizontally, vertically, or diagonally. You could also rotate the images by 90, 180, or 270 degrees. You could also crop the images to different sizes and shapes.\n",
    "\n",
    "**9. What is DECLINE IN LEARNING RATE?**\n",
    "\n",
    "The learning rate is the rate at which the weights of the model are updated during training. A high learning rate can help the model to learn quickly, but it can also lead to overfitting. A low learning rate can help the model to avoid overfitting, but it can also make the training process slower.\n",
    "\n",
    "Overfitting occurs when the model learns the training data too well and is unable to generalize to new data. This can happen when the learning rate is too high, or when the model is too complex.\n",
    "\n",
    "To prevent overfitting, you can use a technique called **learning rate decay**. Learning rate decay is a method of gradually decreasing the learning rate over time. This helps the model to learn the training data while also avoiding overfitting.\n",
    "\n",
    "**What does EARLY STOPPING CRITERIA mean?**\n",
    "\n",
    "Early stopping is a technique that can be used to prevent overfitting by stopping the training process early. Early stopping works by monitoring the performance of the model on a validation dataset. If the performance of the model on the validation dataset stops improving, the training process is stopped.\n",
    "\n",
    "This helps to ensure that the model is not overfitting to the training data. Overfitting occurs when the model learns the training data too well and is unable to generalize to new data. Early stopping can help to prevent overfitting by stopping the training process before the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fef03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
